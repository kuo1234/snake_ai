# 貝爾曼方程在貪吃蛇AI中的應用

## 理論背景

### 什麼是貝爾曼方程？

貝爾曼方程是動態規劃和強化學習的基礎，由理查德·貝爾曼在1950年代提出。在Q-learning中，貝爾曼方程的形式為：

```
Q(s,a) = R(s,a) + γ * max(Q(s',a'))
```

其中：
- `Q(s,a)` = 在狀態s執行動作a的價值
- `R(s,a)` = 即時獎勵
- `γ` = 折扣因子 (0 ≤ γ ≤ 1)
- `s'` = 下一個狀態
- `max(Q(s',a'))` = 下一狀態所有可能動作的最大Q值

### Q-learning更新公式

在實際實現中，我們使用以下更新公式：

```
Q(s,a) = Q(s,a) + α * [R + γ * max(Q(s',a')) - Q(s,a)]
```

其中 `α` 是學習率 (learning rate)。

## 在貪吃蛇中的實現

### 1. 狀態表示 (State Representation)

我們將複雜的遊戲狀態簡化為12個布林特徵：

```python
狀態特徵：
[危險_上, 危險_下, 危險_左, 危險_右,    # 4個危險檢測
 食物_上, 食物_下, 食物_左, 食物_右,    # 4個食物方向
 方向_上, 方向_下, 方向_左, 方向_右]    # 4個當前方向
```

這種表示方法的優點：
- **簡化狀態空間**: 從數千萬個可能狀態降至數百個
- **局部感知**: 只關注蛇頭周圍的immediate環境
- **可解釋性**: 每個特徵都有明確的物理意義

### 2. 動作空間 (Action Space)

```python
動作映射：
0 = UP    (向上)
1 = LEFT  (向左) 
2 = RIGHT (向右)
3 = DOWN  (向下)
```

### 3. 獎勵函數 (Reward Function)

```python
def calculate_reward(self, game, done, info):
    if done:
        if len(game.snake) == game.grid_size:
            return 100    # 完美遊戲
        else:
            return -100   # 遊戲失敗
    elif info['food_obtained']:
        return 50         # 吃到食物
    else:
        reward = 1        # 存活獎勵
        
        # 距離獎勵/懲罰
        if moving_towards_food:
            reward += 2
        elif moving_away_from_food:
            reward -= 1
            
        return reward
```

### 4. 貝爾曼方程更新過程

```python
def update_q_table(self, state, action, reward, next_state, done):
    current_q = self.get_q_values(state)[action]
    
    if done:
        target_q = reward  # 終端狀態
    else:
        # 貝爾曼方程：R + γ * max(Q(s',a'))
        next_q_values = self.get_q_values(next_state)
        target_q = reward + self.discount_factor * np.max(next_q_values)
    
    # 更新Q值：Q(s,a) += α * [target - Q(s,a)]
    new_q = current_q + self.learning_rate * (target_q - current_q)
    self.q_table[state][action] = new_q
```

## 超參數調優

### 1. 學習率 (Learning Rate, α)

- **高學習率 (0.2-0.5)**: 快速學習，但可能不穩定
- **中等學習率 (0.05-0.15)**: 平衡的選擇
- **低學習率 (0.01-0.05)**: 穩定但學習緩慢

### 2. 折扣因子 (Discount Factor, γ)

- **高折扣 (0.95-0.99)**: 重視長期獎勵，適合需要規劃的遊戲
- **中等折扣 (0.8-0.95)**: 平衡即時和未來獎勵
- **低折扣 (0.5-0.8)**: 重視即時獎勵

### 3. 探索策略 (ε-greedy)

```python
初始探索率: ε = 1.0    # 100% 隨機探索
探索衰減: ε *= 0.995   # 逐步減少探索
最小探索率: ε ≥ 0.01   # 保持少量探索
```

## 學習過程分析

### 階段1: 隨機探索 (Episodes 0-200)
- 高探索率導致隨機行為
- Q值初始化和基礎學習
- 分數較低但在積累經驗

### 階段2: 策略形成 (Episodes 200-800) 
- 探索率降低，開始利用學到的策略
- Q值趨於穩定
- 分數快速提升

### 階段3: 策略優化 (Episodes 800+)
- 低探索率，主要利用最佳策略
- 微調Q值以處理邊緣情況
- 分數達到穩定高水平

## 性能指標

### 我們的實驗結果：

| 訓練模式 | 棋盤大小 | 訓練回合 | 最終平均分數 | Q表大小 |
|---------|---------|---------|-------------|---------|
| 快速    | 6x6     | 500     | 90          | ~230    |
| 標準    | 8x8     | 2000    | 160         | 256     |
| 深度    | 10x10   | 5000    | 預期>300    | >500    |

### 分析：
- 8x8棋盤理論最高分數為 (8×8-3)×10 = 610分
- 我們的AI達到160分，約為理論最高分的26%
- 考慮到狀態簡化和探索需求，這是很好的結果

## 優化建議

### 1. 狀態表示改進
- 增加更多局部特徵（如蛇身密度）
- 考慮多步預測信息
- 使用更精細的距離編碼

### 2. 獎勵函數調優
- 動態調整獎勵權重
- 增加路徑效率獎勵
- 懲罰重複訪問區域

### 3. 算法改進
- 實現Double Q-learning減少過估計
- 使用優先經驗回放
- 考慮多智能體競爭環境

## 貝爾曼方程的直觀理解

貝爾曼方程告訴我們：

> "一個狀態-動作對的真正價值 = 即時獲得的獎勵 + 未來可能獲得的最大價值"

在貪吃蛇中：
- **即時獎勵**: 吃到食物 (+50)、存活 (+1)、死亡 (-100)
- **未來價值**: 執行動作後到達的新位置的最大Q值

這樣，AI不僅考慮當前的收益，還會考慮長遠的影響，學會做出戰略性決策。

## 實際應用價值

這個項目展示了貝爾曼方程在實際問題中的應用：

1. **問題分解**: 將複雜遊戲分解為狀態-動作-獎勵序列
2. **最優化**: 通過迭代更新找到最優策略  
3. **泛化能力**: 學到的策略可以應對未見過的遊戲情況
4. **實時決策**: 快速計算最佳動作

這些概念可以應用到機器人導航、資源分配、金融決策等多個領域。