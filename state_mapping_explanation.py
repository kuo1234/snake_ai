"""
Q-table 狀態映射可視化
展示同一個狀態特徵對應不同絕對位置的例子
"""

def visualize_state_mapping():
    print("="*60)
    print("🎯 Q-table 狀態映射可視化")
    print("="*60)
    
    print("""
    關鍵概念：我們的AI不關心絕對位置，只關心相對關係！
    
    🔍 狀態特徵 vs 絕對位置的對比：
    
    ❌ 如果按絕對位置 (錯誤方法)：
    ┌───────────────────────────┐
    │  每個 (蛇頭位置, 食物位置) │
    │  都需要單獨的Q值          │
    │  → 需要64×64 = 4096個Q表  │
    └───────────────────────────┘
    
    ✅ 我們的相對特徵方法 (正確方法)：
    ┌─────────────────────────────────┐
    │  相同的相對關係 = 同一個狀態    │
    │  → 只需要~256個有意義的狀態     │
    └─────────────────────────────────┘
    """)
    
    print("\n🌟 具體例子：")
    print("-" * 50)
    
    # 例子1：食物在右上方的不同情況
    scenarios_same_state = [
        {"snake_head": (3, 2), "food": (1, 4), "desc": "蛇在(3,2)，食物在(1,4)"},
        {"snake_head": (5, 1), "food": (2, 3), "desc": "蛇在(5,1)，食物在(2,3)"},
        {"snake_head": (4, 3), "food": (2, 6), "desc": "蛇在(4,3)，食物在(2,6)"},
    ]
    
    print("📋 這些情況都對應「相同的狀態特徵」：")
    for i, scenario in enumerate(scenarios_same_state, 1):
        head = scenario["snake_head"]
        food = scenario["food"]
        
        # 計算相對關係
        food_up = food[0] < head[0]      # 食物在上方
        food_right = food[1] > head[1]   # 食物在右方
        
        print(f"  {i}. {scenario['desc']}")
        print(f"     → 食物在上方: {food_up}, 食物在右方: {food_right}")
    
    print(f"\n   🎯 這些情況都會查找同一個Q值！")
    print(f"   因為它們的相對關係特徵完全相同。")
    
    print("\n" + "="*60)
    print("💡 為什麼這樣設計更聰明？")
    print("="*60)
    
    print("""
    1. 🚀 學習效率：
       當AI在任何一種情況下學會「食物在右上方時向右移動」，
       這個知識立即適用於所有類似的相對位置關係！
    
    2. 🎯 泛化能力：
       AI學到的策略不局限於特定位置，
       而是通用的相對位置策略。
    
    3. 💾 空間效率：
       不需要為每個絕對位置組合單獨存儲Q值，
       大幅減少了所需的記憶體空間。
    
    4. ⚡ 速度優勢：
       更少的狀態意味著更快的查找和更新速度。
    """)

def show_actual_qtable_structure():
    print("\n" + "="*60) 
    print("📊 實際的Q-table結構")
    print("="*60)
    
    print("""
    我們的Q-table實際上是這樣的字典結構：
    
    q_table = {
        # 狀態特徵元組                          # 4個動作的Q值
        (0,0,1,0,1,0,0,1,0,0,0,1): [10.5, -2.3, 8.7, 1.2],
        (1,0,0,0,0,1,0,0,1,0,0,0): [15.2, 12.1, -5.4, 9.8],
        (0,1,0,1,0,0,1,0,0,1,0,0): [-1.2, 18.4, 7.6, -3.1],
        # ... 更多狀態
    }
    
    查找過程：
    1️⃣ 獲取當前遊戲狀態 → 計算12維特徵向量
    2️⃣ 用特徵向量作為key → 查找對應的Q值陣列
    3️⃣ 選擇Q值最高的動作 → 執行該動作
    
    例如：
    狀態 (0,0,1,0,1,0,0,1,0,0,0,1) 代表：
    - 危險：左方有危險，其他方向安全
    - 食物：在上方和右方
    - 方向：當前向右移動
    
    對應Q值 [10.5, -2.3, 8.7, 1.2]：
    - UP: 10.5    ← 最高！AI會選擇向上
    - LEFT: -2.3  
    - RIGHT: 8.7
    - DOWN: 1.2
    """)

def compare_with_absolute_positioning():
    print("\n" + "="*60)
    print("⚖️  相對特徵 vs 絕對位置的比較")
    print("="*60)
    
    print("""
    🔴 絕對位置方法 (我們沒有使用)：
    ├─ 狀態表示：(蛇頭行, 蛇頭列, 食物行, 食物列, 方向)  
    ├─ 狀態數量：8×8×8×8×4 = 16,384 種可能狀態
    ├─ Q表大小：16,384 × 4 = 65,536 個Q值
    ├─ 問題：每個位置組合都需要單獨學習
    └─ 結果：學習緩慢，需要大量訓練數據
    
    🟢 相對特徵方法 (我們使用的)：
    ├─ 狀態表示：12個布林特徵 (危險+食物方向+當前方向)
    ├─ 狀態數量：理論2^12=4096，實際~256個有意義狀態
    ├─ Q表大小：256 × 4 = 1,024 個Q值  
    ├─ 優點：相似情況共享學習經驗
    └─ 結果：快速學習，高效泛化！
    
    📈 性能對比：
    
    指標              絕對位置法    相對特徵法
    ──────────────────────────────────────
    Q表大小           65,536       1,024      
    學習速度           慢          快         
    泛化能力           差          優秀       
    記憶體需求          大          小         
    訓練數據需求        多          少         
    """)

if __name__ == "__main__":
    visualize_state_mapping()
    show_actual_qtable_structure() 
    compare_with_absolute_positioning()
    
    print("\n" + "🎉" * 20)
    print("總結：我們的Q-table是基於智能的相對特徵設計的，")
    print("而不是簡單的絕對位置映射！這就是為什麼AI能")
    print("快速學會玩貪吃蛇並達到專家級表現的關鍵原因。")
    print("🎉" * 20)