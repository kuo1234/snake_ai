"""
PPO V3: Curriculum Learning èª²ç¨‹å­¸ç¿’ç‰ˆæœ¬ï¼ˆV3 å„ªåŒ–çå‹µï¼‰
çµåˆ V3 çš„èª²ç¨‹å„ªåŒ–çå‹µ + èª²ç¨‹å­¸ç¿’ï¼ˆå¾æ˜“åˆ°é›£ï¼‰

V3 çå‹µå„ªåŒ–é‡é»ï¼š
âœ“ å¾é‚Šè§’é–‹å§‹ï¼šçå‹µåœ¨é‚Šç·£ç§»å‹•ï¼ˆ+0.3 * é‚Šç·£ä¿‚æ•¸ï¼‰ï¼Œé‚Šç·£æ™‚é–“ä½”æ¯”è¿½è¹¤
âœ“ ä¿æŒè€å¿ƒï¼šé™ä½è·é›¢çå‹µæ€¥è¿«æ€§ï¼ˆ+0.3/-0.2ï¼ŒåŸç‚º +1.0/-0.5ï¼‰ï¼Œæé«˜ç”Ÿå­˜çå‹µï¼ˆ+0.2ï¼‰
âœ“ å–„ç”¨è½‰å½ï¼šçå‹µé¿é–‹ç¢°æ’çš„æˆ°è¡“è½‰å½ï¼ˆ+0.5ï¼‰ï¼Œè¿½è¹¤æˆåŠŸè½‰å½æ•¸
âœ“ ç©ºé–“ç®¡ç†ï¼šä¸­å¿ƒé–‹æ”¾çå‹µï¼ˆ+0.5ï¼‰ï¼Œé™ä½é™·é˜±æ‡²ç½°ï¼ˆ-1.5ï¼‰
âœ“ æº«å’Œæ‡²ç½°ï¼šé™ä½æ­»äº¡æ‡²ç½°ï¼ˆ-10 to -30ï¼ŒåŸç‚º -20 to -50ï¼‰ï¼Œå°ˆæ³¨å­¸ç¿’è€Œéææ‡¼

èª²ç¨‹è¨­è¨ˆï¼š
éšæ®µ 1: 6x6 å°æ£‹ç›¤ï¼ˆæ–°æ‰‹æ‘ï¼‰- å­¸ç¿’å¾é‚Šè§’é–‹å§‹ã€ä¿æŒè€å¿ƒã€å–„ç”¨è½‰å½
éšæ®µ 2: 8x8 æ¨™æº–æ£‹ç›¤ï¼ˆé€²éšç­ï¼‰- åœ¨ä¸­ç­‰ç©ºé–“ä¸­å„ªåŒ–ç­–ç•¥
éšæ®µ 3: 10x10 å›°é›£æ£‹ç›¤ï¼ˆæŒ‘æˆ°ç­ï¼‰- è™•ç†æ›´è¤‡é›œçš„ç©ºé–“è¦åŠƒ
éšæ®µ 4: 12x12 æ¥µé›£æ£‹ç›¤ï¼ˆå¤§å¸«ç­ï¼‰- æœ€çµ‚æŒ‘æˆ°

æ¯å€‹éšæ®µéƒ½æœƒï¼š
1. ç¹¼æ‰¿ä¸Šä¸€éšæ®µçš„æ¨¡å‹ï¼ˆTransfer Learningï¼‰
2. è¨­å®šç•¢æ¥­æ¨™æº–ï¼ˆå¹³å‡åˆ†æ•¸é”æ¨™ï¼‰
3. ä½¿ç”¨ V3 çš„èª²ç¨‹å„ªåŒ–çå‹µï¼ˆå°ˆç‚ºå°åœ°åœ–è¨­è¨ˆï¼‰

æ–°å¢ç‰¹æ€§ï¼š
- V3 ç’°å¢ƒï¼š20ç¶­è§€å¯Ÿç©ºé–“ï¼ˆå¢åŠ é‚Šç·£è·é›¢ã€è›‡é•·æ¯”ä¾‹ã€å¯ç”¨ç©ºé–“æ¯”ä¾‹ï¼‰
- æ›´å¤§çš„ç¥ç¶“ç¶²è·¯ï¼š[256, 256, 128] å–ä»£ [128, 128, 64]
- æ›´é•·çš„è¨“ç·´æ™‚é–“ï¼šStage 1 å¢åŠ åˆ° 50è¬æ­¥ï¼ˆå¾ 30è¬ï¼‰
- è‡ªå‹•éšæ®µåˆ‡æ›ï¼šé”æ¨™å¾Œè‡ªå‹•å‡ç´šï¼ˆç„¡éœ€æ‰‹å‹•ç¢ºèªï¼‰
- å®Œæ•´çš„è¨“ç·´è¿½è¹¤å’Œæ—¥èªŒ
"""

import os
import sys
import argparse
import time
import numpy as np
import torch
from typing import Optional, Dict, List
from stable_baselines3 import PPO
from stable_baselines3.common.vec_env import DummyVecEnv, SubprocVecEnv
from stable_baselines3.common.callbacks import BaseCallback, EvalCallback, CheckpointCallback
from stable_baselines3.common.monitor import Monitor
from stable_baselines3.common.logger import configure

# å°å…¥ V3 ç’°å¢ƒï¼ˆèª²ç¨‹å„ªåŒ–ç‰ˆï¼‰
from envs.gym_snake_env_v3 import GymSnakeEnvV3


class CurriculumStage:
    """èª²ç¨‹éšæ®µå®šç¾©"""
    def __init__(self, name: str, board_size: int, timesteps: int, 
                 graduation_score: float, description: str):
        self.name = name
        self.board_size = board_size
        self.timesteps = timesteps
        self.graduation_score = graduation_score
        self.description = description
        self.completed = False
        self.best_score = 0.0
        self.current_score = 0.0


class CurriculumManager:
    """èª²ç¨‹ç®¡ç†å™¨ - The Curriculum Coach"""
    
    def __init__(self, base_dir: str = "models/ppo_snake_v3_curriculum"):
        self.base_dir = base_dir
        os.makedirs(base_dir, exist_ok=True)
        
        # å®šç¾©èª²ç¨‹éšæ®µï¼ˆä½¿ç”¨ V3 å„ªåŒ–å¾Œçš„çå‹µï¼‰
        self.stages = [
            CurriculumStage(
                name="Stage1_Novice",
                board_size=6,
                timesteps=500000,  # 50è¬æ­¥ï¼ˆå¢åŠ è¨“ç·´æ™‚é–“ï¼Œå­¸ç¿’é‚Šç·£ç­–ç•¥ï¼‰
                graduation_score=25.0,  # 6x6 æœ€é«˜35åˆ†ï¼Œè¦æ±‚25åˆ†ç•¢æ¥­ï¼ˆæé«˜æ¨™æº–ï¼‰
                description="æ–°æ‰‹æ‘ï¼š6x6å°æ£‹ç›¤ï¼Œå­¸ç¿’å¾é‚Šè§’é–‹å§‹ã€ä¿æŒè€å¿ƒã€å–„ç”¨è½‰å½"
            ),
            CurriculumStage(
                name="Stage2_Intermediate",
                board_size=8,
                timesteps=500000,  # 50è¬æ­¥
                graduation_score=40.0,  # 8x8 æœ€é«˜63åˆ†ï¼Œè¦æ±‚40åˆ†ç•¢æ¥­ï¼ˆæé«˜æ¨™æº–ï¼‰
                description="é€²éšç­ï¼š8x8æ¨™æº–æ£‹ç›¤ï¼Œå„ªåŒ–ä¸­ç­‰ç©ºé–“ç­–ç•¥"
            ),
            CurriculumStage(
                name="Stage3_Advanced",
                board_size=10,
                timesteps=800000,  # 80è¬æ­¥
                graduation_score=50.0,  # 10x10 æœ€é«˜99åˆ†ï¼Œè¦æ±‚50åˆ†ç•¢æ¥­
                description="æŒ‘æˆ°ç­ï¼š10x10å›°é›£æ£‹ç›¤ï¼Œè¤‡é›œç©ºé–“è¦åŠƒ"
            ),
            CurriculumStage(
                name="Stage4_Master",
                board_size=12,
                timesteps=1000000,  # 100è¬æ­¥
                graduation_score=70.0,  # 12x12 æœ€é«˜143åˆ†ï¼Œè¦æ±‚70åˆ†ç•¢æ¥­
                description="å¤§å¸«ç­ï¼š12x12æ¥µé›£æ£‹ç›¤ï¼Œçµ‚æ¥µæŒ‘æˆ°"
            )
        ]
        
        self.current_stage_idx = 0
        self.training_log = []
    
    @property
    def current_stage(self) -> CurriculumStage:
        """ç²å–ç•¶å‰éšæ®µ"""
        return self.stages[self.current_stage_idx]
    
    def check_graduation(self, mean_score: float) -> bool:
        """æª¢æŸ¥æ˜¯å¦é”åˆ°ç•¢æ¥­æ¨™æº–"""
        stage = self.current_stage
        stage.current_score = mean_score
        stage.best_score = max(stage.best_score, mean_score)
        
        if mean_score >= stage.graduation_score:
            stage.completed = True
            print(f"\nğŸ“ æ­å–œï¼{stage.name} ç•¢æ¥­äº†ï¼")
            print(f"   ç•¢æ¥­åˆ†æ•¸: {mean_score:.1f} (æ¨™æº–: {stage.graduation_score})")
            return True
        return False
    
    def advance_stage(self) -> bool:
        """é€²å…¥ä¸‹ä¸€éšæ®µ"""
        if self.current_stage_idx < len(self.stages) - 1:
            self.current_stage_idx += 1
            print(f"\nğŸ“ˆ å‡ç´šåˆ° {self.current_stage.name}!")
            print(f"   {self.current_stage.description}")
            return True
        else:
            print(f"\nğŸ† æ­å–œå®Œæˆæ‰€æœ‰èª²ç¨‹éšæ®µï¼")
            return False
    
    def get_model_path(self, stage: Optional[CurriculumStage] = None) -> str:
        """ç²å–æ¨¡å‹ä¿å­˜è·¯å¾‘"""
        if stage is None:
            stage = self.current_stage
        return os.path.join(self.base_dir, stage.name, "model")
    
    def get_best_model_path(self, stage: Optional[CurriculumStage] = None) -> str:
        """ç²å–æœ€ä½³æ¨¡å‹è·¯å¾‘"""
        if stage is None:
            stage = self.current_stage
        return os.path.join(self.base_dir, stage.name, "best_model")
    
    def save_progress(self):
        """ä¿å­˜è¨“ç·´é€²åº¦"""
        progress_file = os.path.join(self.base_dir, "curriculum_progress.txt")
        with open(progress_file, 'w', encoding='utf-8') as f:
            f.write("=== PPO V3 Curriculum Learning Progress ===\n\n")
            for i, stage in enumerate(self.stages):
                status = "âœ“ å·²å®Œæˆ" if stage.completed else "â—‹ é€²è¡Œä¸­" if i == self.current_stage_idx else "- æœªé–‹å§‹"
                f.write(f"{status} {stage.name} ({stage.board_size}x{stage.board_size})\n")
                f.write(f"   æè¿°: {stage.description}\n")
                f.write(f"   æœ€ä½³åˆ†æ•¸: {stage.best_score:.1f}\n")
                f.write(f"   ç•¢æ¥­æ¨™æº–: {stage.graduation_score}\n")
                f.write(f"   è¨“ç·´æ­¥æ•¸: {stage.timesteps}\n\n")
            
            f.write(f"\nç•¶å‰éšæ®µ: {self.current_stage.name}\n")


class CurriculumCallback(BaseCallback):
    """èª²ç¨‹å­¸ç¿’å›èª¿ - ç›£æ§è¨“ç·´é€²åº¦ä¸¦æ±ºå®šæ˜¯å¦ç•¢æ¥­"""
    
    def __init__(self, curriculum_manager: CurriculumManager, 
                 eval_freq: int = 10000, n_eval_episodes: int = 20,
                 verbose: int = 1):
        super().__init__(verbose)
        self.curriculum_manager = curriculum_manager
        self.eval_freq = eval_freq
        self.n_eval_episodes = n_eval_episodes
        self.eval_count = 0
    
    def _on_step(self) -> bool:
        # æ¯ eval_freq æ­¥è©•ä¼°ä¸€æ¬¡
        if self.n_calls % self.eval_freq == 0:
            self.eval_count += 1
            mean_score = self._evaluate_agent()
            
            stage = self.curriculum_manager.current_stage
            print(f"\n[è©•ä¼° #{self.eval_count}] {stage.name} - å¹³å‡åˆ†æ•¸: {mean_score:.1f} / {stage.graduation_score}")
            
            # æª¢æŸ¥æ˜¯å¦ç•¢æ¥­
            if self.curriculum_manager.check_graduation(mean_score):
                # ä¿å­˜ç•¢æ¥­æ¨¡å‹
                model_path = self.curriculum_manager.get_model_path()
                self.model.save(model_path)
                print(f"   âœ“ æ¨¡å‹å·²ä¿å­˜: {model_path}")
            
            # ä¿å­˜é€²åº¦
            self.curriculum_manager.save_progress()
        
        return True
    
    def _evaluate_agent(self) -> float:
        """è©•ä¼°ç•¶å‰æ™ºèƒ½é«”"""
        stage = self.curriculum_manager.current_stage
        stage_num = self.curriculum_manager.current_stage_idx + 1
        env = GymSnakeEnvV3(board_size=stage.board_size, render_mode=None, stage=stage_num)
        
        scores = []
        for _ in range(self.n_eval_episodes):
            obs, info = env.reset()
            done = False
            while not done:
                action, _ = self.model.predict(obs, deterministic=True)
                obs, reward, terminated, truncated, info = env.step(action)
                done = terminated or truncated
            scores.append(info.get('score', 0))
        
        env.close()
        return np.mean(scores)


def create_v3_model(board_size: int, base_model: Optional[PPO] = None, 
                   log_dir: Optional[str] = None, device: str = 'auto', stage: int = 1) -> PPO:
    """å‰µå»º V3 å¢å¼·æ¨¡å‹ï¼ˆæ”¯æŒåŠ¨æ€è¯¾ç¨‹å¥–åŠ±ï¼‰
    
    ç›¸æ¯” V2 çš„æ”¹é€²ï¼š
    - æ›´å¤§çš„ç¶²è·¯ï¼š[256, 256, 128] vs V2 çš„ [128, 128, 64]
    - æ›´å¤šæ¢ç´¢ï¼šlearning_rate ç¨é«˜
    - æ”¯æŒé·ç§»å­¸ç¿’ï¼šå¯ä»¥è¼‰å…¥å‰ä¸€éšæ®µçš„æ¨¡å‹
    - å‹•æ…‹èª²ç¨‹ï¼šStage 1-4 å‹•æ…‹çå‹µä¿‚æ•¸
    
    Args:
        board_size: æ¿å­å¤§å°
        base_model: ç”¨æ–¼é·ç§»å­¸ç¿’çš„åŸºç¤æ¨¡å‹
        log_dir: æ—¥èªŒç›®éŒ„
        device: 'cpu', 'cuda', æˆ– 'auto'
        stage: è¨“ç·´éšæ®µ (1-4)
    """
    
    # æ ¹æ®æ¿å­å¤§å°å’Œstageå†³å®šè¯¾ç¨‹é˜¶æ®µï¼ˆå‘åå…¼å®¹ï¼‰
    curriculum_stage = "conservative" if stage == 1 else "aggressive"
    
    # å‰µå»ºç’°å¢ƒ
    def make_env():
        env = GymSnakeEnvV3(
            board_size=board_size, 
            render_mode=None,
            curriculum_stage=curriculum_stage,
            stage=stage  # NEW: Pass stage parameter
        )
        env = Monitor(env)
        return env
    
    env = DummyVecEnv([make_env])
    
    # æ¨¡å‹è¶…åƒæ•¸ï¼ˆå¢å¼·ç‰ˆï¼‰
    policy_kwargs = dict(
        net_arch=dict(
            pi=[256, 256, 128],  # Policy network: æ›´å¤§ï¼
            vf=[256, 256, 128]   # Value network: æ›´å¤§ï¼
        ),
        activation_fn=torch.nn.ReLU
    )
    
    # å¦‚æœæœ‰å‰ä¸€éšæ®µçš„æ¨¡å‹ï¼Œé€²è¡Œé·ç§»å­¸ç¿’
    if base_model is not None:
        print(f"ğŸ”„ é·ç§»å­¸ç¿’ï¼šè¼‰å…¥å‰ä¸€éšæ®µçš„æ¨¡å‹æ¬Šé‡...")
        # å‰µå»ºæ–°æ¨¡å‹ä½†ä½¿ç”¨èˆŠæ¨¡å‹çš„éƒ¨åˆ†åƒæ•¸
        model = PPO(
            "MlpPolicy",
            env,
            policy_kwargs=policy_kwargs,
            learning_rate=3e-4,
            n_steps=2048,
            batch_size=128,  # å¢åŠ  batch size
            n_epochs=15,     # æ›´å¤š epochs
            gamma=0.995,
            gae_lambda=0.95,
            clip_range=0.2,
            ent_coef=0.01,
            vf_coef=0.5,
            max_grad_norm=0.5,
            verbose=1,
            device=device,
            tensorboard_log=f"./logs/ppo_v3_tensorboard/"
        )
        
        # å˜—è©¦è¤‡è£½éƒ¨åˆ†æ¬Šé‡ï¼ˆå¦‚æœæ¶æ§‹å…¼å®¹ï¼‰
        try:
            # é€™è£¡å¯ä»¥æ‰‹å‹•è¤‡è£½æŸäº›å±¤çš„æ¬Šé‡
            # ç°¡å–®ç‰ˆæœ¬ï¼šè®“å®ƒå¾é ­å­¸ç¿’ï¼Œä½†æœ‰ç¶“é©—åŠ é€Ÿ
            print("   æ–°éšæ®µå°‡åˆ©ç”¨å‰éšæ®µçš„å­¸ç¿’ç¶“é©—")
        except Exception as e:
            print(f"   ç„¡æ³•å®Œå…¨é·ç§»æ¬Šé‡ï¼Œå¾é ­é–‹å§‹: {e}")
    
    else:
        # å¾é›¶é–‹å§‹
        model = PPO(
            "MlpPolicy",
            env,
            policy_kwargs=policy_kwargs,
            learning_rate=3e-4,
            n_steps=2048,
            batch_size=128,
            n_epochs=15,
            gamma=0.995,
            gae_lambda=0.95,
            clip_range=0.2,
            ent_coef=0.01,
            vf_coef=0.5,
            max_grad_norm=0.5,
            verbose=1,
            device=device,
            tensorboard_log=f"./logs/ppo_v3_tensorboard/"
        )
    
    return model


def train_curriculum(device: str = 'auto', start_stage: int = 0,
                    n_envs: int = 8, skip_graduation: bool = False):
    """åŸ·è¡Œèª²ç¨‹å­¸ç¿’è¨“ç·´
    
    Args:
        device: 'cpu', 'cuda', æˆ– 'auto'
        start_stage: å¾å“ªå€‹éšæ®µé–‹å§‹ï¼ˆ0=ç¬¬ä¸€éšæ®µï¼‰
        n_envs: ä¸¦è¡Œç’°å¢ƒæ•¸é‡
        skip_graduation: æ˜¯å¦è·³éç•¢æ¥­æª¢æŸ¥ï¼ˆå¼·åˆ¶å®Œæˆæ‰€æœ‰timestepsï¼‰
    """
    
    print("="*70)
    print("ğŸ“ PPO V3: Curriculum Learning (èª²ç¨‹å­¸ç¿’)")
    print("="*70)
    print(f"è¨­å‚™: {device}")
    print(f"ä¸¦è¡Œç’°å¢ƒ: {n_envs}")
    print(f"èµ·å§‹éšæ®µ: {start_stage}")
    print("="*70)
    
    # å‰µå»ºèª²ç¨‹ç®¡ç†å™¨
    curriculum = CurriculumManager()
    curriculum.current_stage_idx = start_stage
    
    # é¡¯ç¤ºèª²ç¨‹è¨ˆåŠƒ
    print("\nğŸ“š èª²ç¨‹è¨ˆåŠƒ:")
    for i, stage in enumerate(curriculum.stages):
        marker = "â†’" if i == start_stage else " "
        print(f"{marker} {stage.name}: {stage.description}")
        print(f"   æ£‹ç›¤: {stage.board_size}x{stage.board_size}, "
              f"è¨“ç·´æ­¥æ•¸: {stage.timesteps:,}, "
              f"ç•¢æ¥­æ¨™æº–: {stage.graduation_score}åˆ†")
    print()
    
    prev_model = None
    
    # æª¢æŸ¥æ˜¯å¦æœ‰å·²è¨“ç·´å¥½çš„å‰ä¸€éšæ®µæ¨¡å‹
    if start_stage > 0:
        prev_stage_idx = start_stage - 1
        prev_stage = curriculum.stages[prev_stage_idx]
        prev_model_path = curriculum.get_best_model_path(prev_stage)
        
        if os.path.exists(prev_model_path + ".zip"):
            print(f"\nğŸ” ç™¼ç¾å·²è¨“ç·´çš„ {prev_stage.name} æ¨¡å‹")
            print(f"   è·¯å¾‘: {prev_model_path}")
            try:
                prev_model = PPO.load(prev_model_path)
                print(f"   âœ“ æˆåŠŸè¼‰å…¥å‰ä¸€éšæ®µæ¨¡å‹ï¼Œå°‡ç”¨æ–¼é·ç§»å­¸ç¿’")
            except Exception as e:
                print(f"   âœ— è¼‰å…¥å¤±æ•—: {e}")
                print(f"   å°‡å¾é ­é–‹å§‹è¨“ç·´")
                prev_model = None
        else:
            print(f"\nâš ï¸  è­¦å‘Š: æ‰¾ä¸åˆ° {prev_stage.name} çš„æ¨¡å‹")
            print(f"   é æœŸè·¯å¾‘: {prev_model_path}")
            print(f"   å»ºè­°å…ˆè¨“ç·´ Stage {prev_stage_idx + 1}")
            response = input(f"   æ˜¯å¦ç¹¼çºŒå¾é ­è¨“ç·´ Stage {start_stage + 1}? (y/n): ")
            if response.lower() != 'y':
                print("   è¨“ç·´å–æ¶ˆ")
                return
    
    # é€éšæ®µè¨“ç·´
    for stage_idx in range(start_stage, len(curriculum.stages)):
        curriculum.current_stage_idx = stage_idx
        stage = curriculum.current_stage
        
        print("\n" + "="*70)
        print(f"ğŸ“– é–‹å§‹ {stage.name}")
        print(f"   {stage.description}")
        print(f"   æ£‹ç›¤å¤§å°: {stage.board_size}x{stage.board_size}")
        print(f"   è¨“ç·´æ­¥æ•¸: {stage.timesteps:,}")
        print(f"   ç•¢æ¥­æ¨™æº–: å¹³å‡åˆ†æ•¸ â‰¥ {stage.graduation_score}")
        print(f"   éšæ®µç·¨è™Ÿ: Stage {stage_idx + 1}")
        print("="*70)
        
        # å‰µå»ºæ¨¡å‹
        print("\nğŸ”§ æº–å‚™æ¨¡å‹...")
        
        # å¦‚æœæ˜¯å¾å‰ä¸€éšæ®µé·ç§»ï¼Œä½¿ç”¨åƒæ•¸éœ‡ç›ªç­–ç•¥
        if prev_model is not None:
            print(f"   ğŸ”„ å¾ Stage {stage_idx} é·ç§»å­¸ç¿’...")
            
            # å‰µå»ºæ–°ç’°å¢ƒï¼ˆé‡è¦ï¼šå‚³å…¥æ–°çš„ stage åƒæ•¸ï¼‰
            model = create_v3_model(stage.board_size, base_model=prev_model, 
                                   device=device, stage=stage_idx + 1)
            
            # === å¼·åŒ–åƒæ•¸éœ‡ç›ª (Enhanced Hyperparameter Shock) ===
            # åŒæ™‚æé«˜å­¸ç¿’ç‡å’Œæ¢ç´¢ç‡ä¾†æ‰“ç ´èˆŠç­–ç•¥
            print(f"   âš¡ å¼·åŒ–åƒæ•¸éœ‡ç›ª: æé«˜å­¸ç¿’ç‡ + æ¢ç´¢ç‡ä»¥æ‰“ç ´èˆŠç­–ç•¥...")
            original_lr = model.learning_rate
            original_ent = model.ent_coef
            
            # éšæ®µ 1: é«˜å­¸ç¿’ç‡ + é«˜æ¢ç´¢ç‡éœ‡ç›ªï¼ˆ150k æ­¥ï¼‰
            model.learning_rate = 3e-4  # æé«˜å­¸ç¿’ç‡
            model.ent_coef = 0.02       # æé«˜æ¢ç´¢ç‡ï¼ˆåŸå§‹ç´„ 0.01ï¼‰
            print(f"      - éœ‡ç›ªéšæ®µ: LR = 3e-4, Entropy = 0.02")
            print(f"      - è¨“ç·´ 150,000 æ­¥ï¼ˆå¼·åˆ¶é‡æ–°æ¢ç´¢ï¼‰")
            
            model.learn(
                total_timesteps=150_000,
                callback=[],
                progress_bar=True,
                reset_num_timesteps=False
            )
            
            # éšæ®µ 2: ä¸­ç­‰å­¸ç¿’ç‡ + ä¸­ç­‰æ¢ç´¢ç‡éæ¸¡ï¼ˆ100k æ­¥ï¼‰
            model.learning_rate = 1.5e-4
            model.ent_coef = 0.015
            print(f"      - éæ¸¡éšæ®µ: LR = 1.5e-4, Entropy = 0.015")
            print(f"      - è¨“ç·´ 100,000 æ­¥ï¼ˆç©©å®šç­–ç•¥ï¼‰")
            
            model.learn(
                total_timesteps=100_000,
                callback=[],
                progress_bar=True,
                reset_num_timesteps=False
            )
            
            # éšæ®µ 3: æ¢å¾©æ­£å¸¸åƒæ•¸
            model.learning_rate = original_lr
            model.ent_coef = original_ent
            print(f"      - ç©©å®šéšæ®µ: LR = {original_lr}, Entropy = {original_ent}")
            print(f"      - ç¹¼çºŒè¨“ç·´...")
            remaining_timesteps = stage.timesteps - 250_000
            
        else:
            # ç¬¬ä¸€éšæ®µï¼Œå¾é ­é–‹å§‹
            model = create_v3_model(stage.board_size, base_model=None, 
                                   device=device, stage=stage_idx + 1)
            remaining_timesteps = stage.timesteps
        
        # è¨­ç½®æ—¥èªŒ
        log_path = os.path.join(curriculum.base_dir, stage.name, "logs")
        os.makedirs(log_path, exist_ok=True)
        model.set_logger(configure(log_path, ["stdout", "csv", "tensorboard"]))
        
        # è¨­ç½®å›èª¿
        checkpoint_dir = os.path.join(curriculum.base_dir, stage.name, "checkpoints")
        os.makedirs(checkpoint_dir, exist_ok=True)
        
        callbacks = [
            CurriculumCallback(
                curriculum_manager=curriculum,
                eval_freq=10000,
                n_eval_episodes=20,
                verbose=1
            ),
            CheckpointCallback(
                save_freq=50000,
                save_path=checkpoint_dir,
                name_prefix=f"{stage.name}_checkpoint"
            )
        ]
        
        # è¨“ç·´
        print(f"\nğŸš€ é–‹å§‹è¨“ç·´ {stage.name}...")
        start_time = time.time()
        
        try:
            model.learn(
                total_timesteps=remaining_timesteps,
                callback=callbacks,
                progress_bar=True,
                reset_num_timesteps=False if prev_model is not None else True
            )
        except KeyboardInterrupt:
            print("\nâš ï¸  è¨“ç·´è¢«ç”¨æˆ¶ä¸­æ–·")
            model.save(curriculum.get_model_path())
            print(f"   æ¨¡å‹å·²ä¿å­˜")
            break
        
        training_time = time.time() - start_time
        print(f"\nâœ“ {stage.name} è¨“ç·´å®Œæˆï¼")
        print(f"   è¨“ç·´æ™‚é–“: {training_time/60:.1f} åˆ†é˜")
        
        # ä¿å­˜æœ€çµ‚æ¨¡å‹
        final_model_path = curriculum.get_model_path()
        model.save(final_model_path)
        print(f"   æœ€çµ‚æ¨¡å‹: {final_model_path}")
        
        # æœ€çµ‚è©•ä¼°
        print(f"\nğŸ“Š {stage.name} æœ€çµ‚è©•ä¼°...")
        final_score = evaluate_model(model, stage.board_size, n_episodes=50)
        stage.best_score = max(stage.best_score, final_score)
        
        # æª¢æŸ¥æ˜¯å¦ç•¢æ¥­
        if not skip_graduation:
            if curriculum.check_graduation(final_score):
                stage.completed = True
                # ä¿å­˜ç‚ºæœ€ä½³æ¨¡å‹
                best_path = curriculum.get_best_model_path()
                model.save(best_path)
                print(f"   âœ“ ç•¢æ¥­æ¨¡å‹ä¿å­˜: {best_path}")
            else:
                # è‡ªå‹•é€²å…¥ä¸‹ä¸€éšæ®µï¼šä¸å†è©¢å•ï¼Œè¨˜éŒ„è¨Šæ¯ä¸¦ç¹¼çºŒ
                print(f"\nâŒ æœªé”ç•¢æ¥­æ¨™æº– ({final_score:.1f} < {stage.graduation_score})")
                print(f"   è‡ªå‹•å‰é€²è‡³ä¸‹ä¸€éšæ®µï¼ˆå·²åœ¨æœ¬éšæ®µå®Œæˆ {stage.timesteps:,} æ­¥è¨“ç·´ï¼‰ã€‚")
        else:
            stage.completed = True
        
        # ä¿å­˜é€²åº¦
        curriculum.save_progress()
        
        # æº–å‚™ä¸‹ä¸€éšæ®µï¼ˆé·ç§»å­¸ç¿’ï¼‰
        if stage_idx < len(curriculum.stages) - 1:
            if stage.completed:
                print(f"\nğŸ“ {stage.name} ç•¢æ¥­ï¼æº–å‚™é€²å…¥ä¸‹ä¸€éšæ®µ...")
            else:
                print(f"\nâ¡ï¸  å°šæœªé”æ¨™ï¼Œä½†å°‡å¸¶è‘—æœ¬éšæ®µçš„æ¬Šé‡ç¹¼çºŒä¸‹ä¸€éšæ®µã€‚")
            # ç„¡è«–æ˜¯å¦ç•¢æ¥­ï¼Œéƒ½ä½¿ç”¨æœ¬éšæ®µè¨“ç·´å¾Œçš„æ¨¡å‹ä½œç‚ºä¸‹ä¸€éšæ®µçš„èµ·é»
            prev_model = model
        
        print("\n")
    
    # è¨“ç·´å®Œæˆ
    print("\n" + "="*70)
    print("ğŸ† èª²ç¨‹å­¸ç¿’è¨“ç·´å®Œæˆï¼")
    print("="*70)
    
    print("\nğŸ“Š å„éšæ®µæˆç¸¾:")
    for stage in curriculum.stages:
        status = "âœ“" if stage.completed else "âœ—"
        print(f"{status} {stage.name}: æœ€ä½³åˆ†æ•¸ {stage.best_score:.1f} / {stage.graduation_score}")
    
    curriculum.save_progress()
    print(f"\nè©³ç´°é€²åº¦å·²ä¿å­˜è‡³: {curriculum.base_dir}/curriculum_progress.txt")


def evaluate_model(model: PPO, board_size: int, n_episodes: int = 20, stage: int = 1) -> float:
    """è©•ä¼°æ¨¡å‹æ€§èƒ½"""
    # æ ¹æ®æ¿å­å¤§å°ç¡®å®šè¯¾ç¨‹é˜¶æ®µï¼ˆå‘åå…¼å®¹ï¼‰
    curriculum_stage = "conservative" if stage == 1 else "aggressive"
    env = GymSnakeEnvV3(board_size=board_size, render_mode=None, 
                       curriculum_stage=curriculum_stage, stage=stage)
    scores = []
    
    for i in range(n_episodes):
        obs, info = env.reset()
        done = False
        while not done:
            action, _ = model.predict(obs, deterministic=True)
            obs, reward, terminated, truncated, info = env.step(action)
            done = terminated or truncated
        scores.append(info.get('score', 0))
    
    env.close()
    mean_score = np.mean(scores)
    std_score = np.std(scores)
    print(f"   è©•ä¼°çµæœ: {mean_score:.1f} Â± {std_score:.1f} (æœ€é«˜: {max(scores)}, æœ€ä½: {min(scores)})")
    return mean_score


def demo_stage(stage_idx: int = -1, n_episodes: int = 5):
    """æ¼”ç¤ºæŸå€‹éšæ®µçš„æ¨¡å‹"""
    curriculum = CurriculumManager()
    
    if stage_idx == -1:
        # æ‰¾åˆ°æœ€å¾Œå®Œæˆçš„éšæ®µ
        for i in range(len(curriculum.stages) - 1, -1, -1):
            model_path = curriculum.get_best_model_path(curriculum.stages[i])
            if os.path.exists(model_path + ".zip"):
                stage_idx = i
                break
    
    if stage_idx < 0 or stage_idx >= len(curriculum.stages):
        print(f"âŒ éšæ®µ {stage_idx} ä¸å­˜åœ¨")
        return
    
    stage = curriculum.stages[stage_idx]
    model_path = curriculum.get_best_model_path(stage)
    
    if not os.path.exists(model_path + ".zip"):
        print(f"âŒ æ‰¾ä¸åˆ°æ¨¡å‹: {model_path}")
        return
    
    print(f"\nğŸ® æ¼”ç¤º {stage.name} ({stage.board_size}x{stage.board_size})")
    print(f"   è¼‰å…¥æ¨¡å‹: {model_path}")
    
    model = PPO.load(model_path)
    # æ ¹æ®é˜¶æ®µç´¢å¼•ç¡®å®š stage å‚æ•°
    stage_num = stage_idx + 1
    curriculum_stage = "conservative" if stage_num == 1 else "aggressive"
    env = GymSnakeEnvV3(board_size=stage.board_size, render_mode="human", 
                       curriculum_stage=curriculum_stage, stage=stage_num)
    
    for episode in range(n_episodes):
        print(f"\nå›åˆ {episode + 1}/{n_episodes}")
        obs, info = env.reset()
        done = False
        step_count = 0
        
        while not done:
            action, _ = model.predict(obs, deterministic=True)
            obs, reward, terminated, truncated, info = env.step(action)
            done = terminated or truncated
            step_count += 1
            env.render()
            time.sleep(0.05)
        
        score = info.get('score', 0)
        print(f"   åˆ†æ•¸: {score}, æ­¥æ•¸: {step_count}")
    
    env.close()


def main():
    parser = argparse.ArgumentParser(description="PPO V3 - Curriculum Learning")
    parser.add_argument('--mode', type=str, default='train', 
                       choices=['train', 'demo', 'eval'],
                       help='æ¨¡å¼: train=è¨“ç·´, demo=æ¼”ç¤º, eval=è©•ä¼°')
    parser.add_argument('--device', type=str, default='auto',
                       choices=['cpu', 'cuda', 'auto'],
                       help='è¨“ç·´è¨­å‚™')
    parser.add_argument('--n-envs', type=int, default=8,
                       help='ä¸¦è¡Œç’°å¢ƒæ•¸é‡')
    parser.add_argument('--start-stage', type=int, default=0,
                       help='é–‹å§‹éšæ®µ (0-3)')
    parser.add_argument('--stage', type=int, default=-1,
                       help='æ¼”ç¤º/è©•ä¼°çš„éšæ®µ (-1=æœ€æ–°)')
    parser.add_argument('--n-episodes', type=int, default=5,
                       help='æ¼”ç¤º/è©•ä¼°çš„å›åˆæ•¸')
    parser.add_argument('--skip-graduation', action='store_true',
                       help='è·³éç•¢æ¥­æª¢æŸ¥ï¼Œå¼·åˆ¶å®Œæˆæ‰€æœ‰timesteps')
    
    args = parser.parse_args()
    
    if args.mode == 'train':
        train_curriculum(
            device=args.device,
            start_stage=args.start_stage,
            n_envs=args.n_envs,
            skip_graduation=args.skip_graduation
        )
    
    elif args.mode == 'demo':
        demo_stage(stage_idx=args.stage, n_episodes=args.n_episodes)
    
    elif args.mode == 'eval':
        curriculum = CurriculumManager()
        stage_idx = args.stage if args.stage >= 0 else len(curriculum.stages) - 1
        stage = curriculum.stages[stage_idx]
        model_path = curriculum.get_best_model_path(stage)
        
        if not os.path.exists(model_path + ".zip"):
            print(f"âŒ æ‰¾ä¸åˆ°æ¨¡å‹: {model_path}")
            return
        
        print(f"\nğŸ“Š è©•ä¼° {stage.name} ({stage.board_size}x{stage.board_size})")
        model = PPO.load(model_path)
        stage_num = stage_idx + 1
        evaluate_model(model, stage.board_size, n_episodes=args.n_episodes, stage=stage_num)


if __name__ == "__main__":
    main()
