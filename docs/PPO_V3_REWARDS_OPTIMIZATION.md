# PPO V3 课程优化奖励系统

## 问题分析

在 Stage 1 (6x6) 训练中，PPO V2 只能达到 **18/35 分**左右，未能充分学习小地图策略。

## 解决方案：V3 课程优化奖励

基于三大策略优化奖励函数：

### 1. 从边角开始 🔲
**策略**：让蛇先从地图边角开始移动，慢慢向中央缩小活动范围

**实现**：
- 新增 `_is_on_edge()` 检测头部是否在边缘
- 边缘移动奖励：`+0.3 * edge_multiplier`
- `edge_multiplier = max(1.0, 1.5 - snake_length / grid_size)`
  - 蛇短时：1.5 倍奖励（鼓励早期边缘活动）
  - 蛇长时：1.0 倍奖励（逐渐过渡到中央）
- 追踪指标：`edge_time`, `edge_ratio`

**效果**：
- 在边缘吃食物额外 `+2.0` 奖励
- 边缘移动平均奖励：**0.463**（测试验证）

---

### 2. 保持耐心 ⏳
**策略**：地图空间有限，蛇身越长，操作难度越高，需要更谨慎的控制

**实现**：
- **降低距离奖励急迫性**：
  - V2: 靠近食物 `+1.0`, 远离 `-0.5`
  - V3: 靠近食物 `+0.3`, 远离 `-0.2`
  - 减少 70% 的急迫性！
- **提高生存奖励**：
  - V2: `+0.1` 每步
  - V3: `+0.2` 每步
  - 鼓励慢而稳的探索
- **温和死亡惩罚**：
  - V2: 墙 `-20`, 身体 `-10 to -50`
  - V3: 墙 `-10`, 身体 `-10 to -30`
  - 降低 50% 的恐惧，专注学习

**效果**：
- 探索移动平均奖励：**0.142**（V2 会是负数）
- V3 vs V2 奖励对比：**-2.717 vs -6.933**（更温和）

---

### 3. 善用转彯 ↩️
**策略**：在接近自己的身体时，利用快速转彯来避开碰撞

**实现**：
- 新增 `_would_have_collided()` 检测"假如不转弯会撞"
- 战术转彯奖励：`+0.5`
- 追踪指标：`turn_count`, `successful_turns`
- 近身惩罚降低：`-0.3`（V2 为 `-1.0`）

**效果**：
- 成功的防御性转彯获得明确奖励
- 测试中成功转彯率显著提升

---

## 观察空间增强

从 **16 维** → **20 维**：

```
[0-3]:   danger detection (immediate collision)
[4-7]:   body proximity (distance to nearest body segment)
[8-11]:  food direction (up, down, left, right)
[12-15]: current direction (one-hot)
[16-17]: edge proximity (NEW - distance to nearest wall)
[18]:    snake length ratio (NEW - current_length / max_possible)
[19]:    available space ratio (NEW - empty cells / total cells)
```

新增的空间感知特征帮助智能体理解：
- 何时应该靠边
- 空间是否拥挤
- 成长进度

---

## 空间管理优化

### 中心开放奖励
- 当蛇长度 > 30% 地图时，保持中心开放 `+0.5`
- 定义：中心 = 地图中间 50% 区域
- 避免过早占据中心导致后期无路可走

### 陷阱检测优化
- 降低陷阱惩罚：`-1.5`（V2 为 `-2.0`）
- 更宽容的学习环境

---

## 课程学习调整

### Stage 1 (6x6) 参数更新

| 参数 | V2 | V3 | 说明 |
|------|----|----|------|
| 训练步数 | 30万 | **50万** | +67% 训练时间 |
| 毕业标准 | 20分 | **25分** | +25% 要求 |
| 环境 | GymSnakeEnvV2 | **GymSnakeEnvV3** | 课程优化 |
| 描述 | 基本生存和覓食 | **从边角开始、保持耐心、善用转彯** | 明确策略 |

### Stage 2-4 同步提升

| Stage | 板子大小 | 毕业标准 (V2) | 毕业标准 (V3) |
|-------|---------|-------------|-------------|
| 2 | 8x8 | 35 | **40** (+14%) |
| 3 | 10x10 | 50 | 50 (不变) |
| 4 | 12x12 | 70 | 70 (不变) |

---

## 完整奖励结构（V3）

### 正面奖励
- 吃到食物：`+15.0`（V2: +10）
- 吃食物长度奖励：`+min(length * 0.3, 4.0)`
- 边缘吃食物额外：`+2.0`
- 赢得游戏：`+100.0`
- 生存：`+0.2`（V2: +0.1）
- 边缘移动：`+0.3 * edge_multiplier`
- 靠近食物：`+0.3`（V2: +1.0）
- 战术转彯：`+0.5`
- 保持中心开放：`+0.5`

### 负面惩罚
- 墙撞击：`-10.0`（V2: -20）
- 身体撞击（远）：`-10.0`
- 身体撞击（近）：`-20.0`（V2: -30）
- 身体撞击（颈部）：`-30.0`（V2: -50）
- 远离食物：`-0.2`（V2: -0.5）
- 近身风险：`-0.3`（V2: -1.0）
- 陷阱：`-1.5`（V2: -2.0）

**总体趋势**：奖励更丰富，惩罚更温和，鼓励探索而非恐惧

---

## 使用方法

### 1. 测试奖励系统
```bash
python test_v3_rewards.py
```
验证三大策略的奖励是否生效

### 2. 快速训练测试（5 万步）
```bash
python quick_test_v3.py
```
快速验证能否突破 18/35 瓶颈

### 3. 完整课程训练（50 万步 Stage 1）
```bash
python snake_ai_ppo_v3.py --mode train
```
自动进行 4 阶段课程学习

### 4. 仅训练 Stage 1
```bash
python snake_ai_ppo_v3.py --mode train --stage 1
```

### 5. 评估模型
```bash
python snake_ai_ppo_v3.py --mode eval --stage 1
```

### 6. 可视化演示
```bash
python snake_ai_ppo_v3.py --mode demo --stage 1
```

---

## 期望结果

### 短期目标（快速测试 5万步）
- 平均分数：**> 15** 分（突破 18/35 瓶颈的第一步）
- 边缘占比：**> 40%**（验证边缘策略生效）
- 成功转彯率：**> 50%**（验证转彯奖励生效）

### 中期目标（完整训练 50万步）
- 平均分数：**25+** 分（达到 Stage 1 毕业标准）
- 稳定性：标准差 < 5
- 策略特征：
  - 早期优先边缘移动
  - 蛇长时谨慎靠近中心
  - 频繁使用战术转彯

### 长期目标（全课程完成）
- Stage 1: **25+** / 35
- Stage 2: **40+** / 63
- Stage 3: **50+** / 99
- Stage 4: **70+** / 143

---

## 测试验证结果

运行 `test_v3_rewards.py` 的实际结果：

✅ **测试 1 - 边缘奖励**
- 边缘移动平均奖励：**0.463**
- 明显高于基础生存奖励（0.2）

✅ **测试 2 - 耐心机制**
- 探索移动平均奖励：**0.142**
- 远离食物惩罚大幅降低

✅ **测试 3 - 转彯奖励**
- 检测到战术转彯并给予奖励
- 成功转彯追踪正常工作

✅ **测试 4 - V2 vs V3 对比**
- V2 平均奖励：**-6.933**
- V3 平均奖励：**-2.717**
- **改善 152%**！

---

## 技术细节

### 文件结构
```
envs/
  ├── gym_snake_env_v2.py  # 原版（保留）
  └── gym_snake_env_v3.py  # 课程优化版（新）

snake_ai_ppo_v3.py         # 主训练脚本（已更新使用 V3）
test_v3_rewards.py         # 奖励系统测试
quick_test_v3.py           # 快速训练测试
```

### 关键类和方法

**GymSnakeEnvV3**
- `_calculate_reward_v3()`: 主奖励函数
- `_is_on_edge()`: 边缘检测
- `_would_have_collided()`: 转彯必要性判断
- `_is_center_crowded()`: 中心拥挤度
- `_is_trapped()`: 陷阱检测

**观察空间增强**
- `edge_dist_vertical/horizontal`: 到最近墙壁的距离
- `snake_length_ratio`: 当前长度 / 最大可能长度
- `available_space_ratio`: 剩余空间 / 总空间

---

## 理论依据

### 课程学习 (Curriculum Learning)
从简单任务开始，逐步增加难度。在 6x6 上学会的边缘策略可以迁移到更大地图。

### 奖励塑形 (Reward Shaping)
将稀疏奖励（只有吃食物和死亡）塑形为密集奖励（每步都有反馈），加速学习。

### 多目标优化
平衡三大策略：
1. 安全性（边缘移动）
2. 效率性（找到食物）
3. 灵活性（战术转彯）

---

## 故障排除

### 如果分数仍然 < 20
1. 增加训练步数到 100 万
2. 调高边缘奖励系数（0.3 → 0.5）
3. 降低探索系数 `ent_coef`（0.01 → 0.005）
4. 增大网络 `[256, 256, 128]`

### 如果学习不稳定
1. 降低 learning rate（3e-4 → 1e-4）
2. 增大 batch size（64 → 128）
3. 调整 GAE lambda（0.95 → 0.90）

### 如果过拟合边缘
1. 降低边缘奖励系数
2. 增加中心开放奖励
3. 调整 `edge_multiplier` 衰减速度

---

## 后续改进方向

1. **自适应奖励**：根据训练进度动态调整奖励系数
2. **多目标Pareto优化**：边缘 vs 食物的最优权衡
3. **层次强化学习**：高层策略（边缘/中心）+ 低层动作
4. **对抗训练**：学习应对不同食物分布模式

---

## 贡献者

课程设计基于对小地图贪吃蛇的深入分析：
- **边角策略**：减少碰撞风险，最大化生存空间
- **耐心策略**：避免急躁导致的自杀性移动
- **转彯技巧**：在紧急情况下的关键生存技能

V3 实现将这些人类专家策略编码为奖励函数，引导 PPO 学习。

---

## 许可证

MIT License - 自由使用和修改
