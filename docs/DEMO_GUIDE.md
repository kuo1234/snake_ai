# 使用 demo_ai.py 觀看 PPO 訓練結果

## 📋 功能說明

`demo_ai.py` 現已支援兩種 AI 方法的演示：

### 選項 1-3: Q-learning 方法（傳統）
- 選項 1: 觀看 Q-learning AI 玩遊戲
- 選項 2: 比較訓練前後的表現
- 選項 3: 分析 Q 表策略

### 選項 4-5: PPO 方法（深度強化學習）🌟
- **選項 4**: 觀看 PPO AI 玩遊戲（圖形界面）
- **選項 5**: Q-learning vs PPO 性能比較

## 🚀 使用方法

### 1. 訓練 PPO 模型

首先需要訓練一個 PPO 模型：

```powershell
# 快速訓練（10 萬步，約 5-10 分鐘）
python snake_ai_ppo.py --mode train --timesteps 100000 --board-size 8
```

訓練完成後，模型會保存在：
- `models/ppo_snake/best_model/best_model.zip` - 最佳模型
- `models/ppo_snake/ppo_snake_final.zip` - 最終模型

### 2. 運行演示程序

```powershell
python demo_ai.py
```

你會看到選單：

```
============================================================
貪吃蛇 AI 演示程序
============================================================

請選擇演示模式:

Q-learning 方法 (傳統)
  1. 觀看 Q-learning AI 遊戲 (圖形界面)
  2. Q-learning 訓練前後比較
  3. Q-learning AI 策略分析

PPO 方法 (深度強化學習) 🌟 推薦
  4. 觀看 PPO AI 遊戲 (圖形界面)
  5. Q-learning vs PPO 性能比較

============================================================

請輸入選擇 (1-5):
```

### 3. 觀看 PPO AI 玩遊戲（選項 4）

輸入 `4` 後會：

1. 自動載入最佳 PPO 模型
2. 開啟 pygame 視窗
3. 顯示 AI 的決策過程

**控制鍵**：
- `ESC`: 退出演示
- `空格`: 暫停/繼續

**顯示資訊**：
```
步驟   1: 動作=DOWN  分數=  0 蛇長=  3
步驟   2: 動作=RIGHT 分數=  0 蛇長=  3
步驟   3: 動作=DOWN  分數=  1 蛇長=  4
...

遊戲結束統計:
  最終分數: 15
  蛇的長度: 18
  總步數: 156
  總獎勵: 125.50
  成功率: 15/63 (23.8%)
```

### 4. 比較兩種方法（選項 5）

輸入 `5` 會自動運行 10 局遊戲，比較：

- Q-learning 的平均分數和最高分
- PPO 的平均分數和最高分
- 兩種方法的性能差異
- 各自的優勢和劣勢

範例輸出：

```
============================================================
Q-learning vs PPO 性能比較
============================================================

測試 Q-learning AI...
✓ 載入 Q-learning 模型成功
  遊戲 1: 分數=12, 步數=89
  遊戲 2: 分數=8, 步數=67
  ...
  平均分數: 10.50
  最高分數: 15

測試 PPO AI...
✓ 載入 PPO 模型成功: models/ppo_snake/best_model/best_model.zip
  遊戲 1: 分數=25, 步數=234
  遊戲 2: 分數=18, 步數=178
  ...
  平均分數: 22.30
  最高分數: 35

============================================================
比較結果總結
============================================================

方法            平均分數      最高分數      優勢
------------------------------------------------------------
Q-learning      10.50        15          傳統方法，穩定
PPO             22.30        35          深度學習，擴展性強

PPO 相比 Q-learning 的改進: +112.4%

============================================================
算法特性比較:
============================================================

Q-learning:
  ✓ 簡單易懂，實現簡單
  ✓ 訓練快速（小棋盤）
  ✗ 狀態空間爆炸（大棋盤）
  ✗ 難以擴展到複雜問題

PPO:
  ✓ 使用深度神經網路，擴展性強
  ✓ 可處理大規模狀態空間
  ✓ 支援多進程並行訓練
  ✓ 現代 RL 標準方法
  ✗ 需要更多訓練時間
  ✗ 需要調整超參數
============================================================
```

## 🎯 PPO 演示的特色

相比 Q-learning 演示，PPO 演示提供：

1. **更詳細的統計**：
   - 總獎勵（累積獎勵）
   - 成功率（當前分數 / 最高分）
   - 每步的蛇長變化

2. **更流暢的遊戲**：
   - 使用神經網路決策更快
   - 動作選擇更平滑

3. **更好的性能**：
   - 通常能獲得更高分數
   - 更少的無效移動

## 🔧 故障排除

### 問題 1: PPO 選項不可用

如果看到：
```
PPO 方法 (需要安裝 stable_baselines3)
  安裝方法: pip install stable-baselines3 torch
```

**解決方法**：
```powershell
pip install stable-baselines3 torch gymnasium
```

### 問題 2: 找不到 PPO 模型

如果看到：
```
警告: 未找到 PPO 預訓練模型
請先訓練模型:
  python snake_ai_ppo.py --mode train --timesteps 100000
```

**解決方法**：
```powershell
# 快速訓練一個模型
python snake_ai_ppo.py --mode train --timesteps 100000 --board-size 8
```

### 問題 3: 遊戲視窗不顯示

**確認**：
- pygame 是否已安裝: `pip install pygame`
- 是否選擇了選項 4（圖形模式）
- 視窗是否被其他程式遮擋

## 📊 觀察 PPO 的學習效果

當你觀看 PPO AI 玩遊戲時，注意觀察：

1. **避障能力**：
   - PPO 能否預測並避開危險
   - 是否會卡在死角

2. **尋食策略**：
   - 是否能高效找到食物
   - 路徑規劃是否合理

3. **長期規劃**：
   - 蛇變長後是否仍能靈活移動
   - 是否會為吃食物製造更多空間

4. **穩定性**：
   - 連續多局的表現是否穩定
   - 最高分和平均分的差距

## 🎓 進階使用

### 比較不同訓練階段的模型

```powershell
# 訓練 10 萬步
python snake_ai_ppo.py --mode train --timesteps 100000

# 觀看效果
python demo_ai.py  # 選擇 4

# 繼續訓練到 50 萬步
python snake_ai_ppo.py --mode train --timesteps 500000

# 再次觀看，比較進步
python demo_ai.py  # 選擇 4
```

### 測試不同棋盤大小

修改 `demo_ai.py` 中的 `board_size` 參數（第 252 行附近）：

```python
board_size = 10  # 改為 6, 8, 10, 12 等
```

## 📈 預期效果

根據訓練程度，PPO AI 的表現：

| 訓練步數 | 棋盤大小 | 預期平均分 | 預期最高分 |
|---------|---------|-----------|-----------|
| 10 萬   | 6×6     | 8-12      | 15-20     |
| 10 萬   | 8×8     | 10-15     | 20-30     |
| 50 萬   | 8×8     | 20-30     | 35-45     |
| 50 萬   | 10×10   | 25-35     | 45-60     |
| 100 萬  | 10×10   | 35-50     | 60-75     |

**注意**：實際效果會因隨機種子和訓練過程而有所不同。

---

現在你可以：
1. 訓練 PPO 模型
2. 使用 `demo_ai.py` 觀看效果
3. 比較不同方法的性能

享受觀看 AI 學習的過程！🐍🤖🎮
