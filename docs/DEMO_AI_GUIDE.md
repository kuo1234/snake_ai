# demo_ai.py 使用指南 (更新版)

## 🎮 功能概覽

`demo_ai.py` 是一個交互式演示程序，讓你可以：
- 觀看訓練好的 AI 玩貪吃蛇遊戲
- 選擇不同的 AI 模型（Q-learning、PPO V1、PPO V2）
- 自訂棋盤大小（6x6 到 12x12）
- 比較不同 AI 方法的效能

---

## 🚀 快速開始

### 基本使用

```bash
python demo_ai.py
```

程序會顯示主選單，讓你選擇不同的演示模式。

---

## 📋 主選單選項

### Q-learning 方法 (傳統)

#### 選項 1: 觀看 Q-learning AI 遊戲
- 可選擇棋盤大小（6x6, 8x8, 10x10, 12x12）
- 顯示 AI 的思考過程（Q 值）
- 支援暫停/繼續（空格鍵）

#### 選項 2: Q-learning 訓練前後比較
- 比較未訓練和訓練後的 AI 表現
- 顯示平均分數改進

#### 選項 3: Q-learning AI 策略分析
- 分析 Q 表大小
- 顯示 Q 值統計
- 找出最有價值的狀態-動作對

### PPO 方法 (深度學習)

#### 選項 4: 觀看 PPO V1 AI 遊戲
- 基礎版 PPO
- 可選擇棋盤大小
- 12-d 觀察空間
- 顯示詳細統計

#### 選項 5: 觀看 PPO V2 AI 遊戲 ⭐ 推薦
- 增強版 PPO（解決碰撞問題）
- 可選擇棋盤大小
- 16-d 觀察空間（含身體距離感知）
- 顯示 near-miss 次數
- 更智能的避障行為

#### 選項 6: Q-learning vs PPO V1 vs PPO V2 三方比較
- 同時測試三種方法
- 可選擇棋盤大小
- 各跑 10 回合
- 顯示詳細統計和改進百分比
- 特性對比分析

#### 選項 7: 選擇特定模型和棋盤大小 (進階)
- 列出所有可用模型
- 讓用戶選擇特定模型文件
- 自訂棋盤大小
- 適合測試不同訓練階段的模型

---

## 🎯 棋盤大小選項

程序會提示你選擇棋盤大小：

| 選項 | 大小 | 難度 | 最高分 |
|-----|------|-----|--------|
| 1 | 6x6 | 簡單 | 35 |
| 2 | 8x8 | 標準 | 63 |
| 3 | 10x10 | 困難 | 99 |
| 4 | 12x12 | 極難 | 143 |

**推薦**: 8x8 (標準大小，平衡難度和性能)

---

## 🎮 操作控制

### 遊戲中
- **ESC 鍵**: 結束演示
- **空格鍵**: 暫停/繼續
- **關閉窗口**: 結束程序

### 選單中
- **輸入數字**: 選擇選項
- **Enter**: 確認選擇
- **Ctrl+C**: 中斷程序

---

## 📊 選項 6 詳細說明（三方比較）

這是最全面的比較模式，會：

### 測試流程
1. 選擇棋盤大小
2. 依序測試三種方法：
   - Q-learning
   - PPO V1
   - PPO V2
3. 每種方法跑 10 回合
4. 收集統計數據

### 輸出內容

#### 1. 個別測試結果
```
測試 Q-learning AI...
  遊戲  1: 分數=  5, 步數= 120
  遊戲  2: 分數=  3, 步數=  87
  ...
  平均分數: 4.20 ± 1.50
```

#### 2. 比較表格
```
方法            平均分數        最高分     最低分     標準差
──────────────────────────────────────────────────────
Q-learning      4.20           8         1         1.50
PPO V1          7.50           12        3         2.30
PPO V2          11.80          24        6         4.20
```

#### 3. 改進分析
```
PPO V1 相比 Q-learning: +78.6%
PPO V2 相比 Q-learning: +181.0%
PPO V2 相比 PPO V1: +57.3%
```

#### 4. 特性對比
- 各方法的優缺點
- 技術特性說明
- 使用場景建議

---

## 📁 模型文件位置

程序會自動搜尋以下位置的模型：

### Q-learning 模型
```
snake_ai_standard.pkl
snake_ai_quick.pkl
snake_ai_final.pkl
```

### PPO V1 模型
```
models/ppo_snake/best_model/best_model.zip
models/ppo_snake/ppo_snake_final.zip
```

### PPO V2 模型
```
models/ppo_snake_v2/best_model/best_model.zip
models/ppo_snake_v2/ppo_snake_v2_final.zip
```

---

## 💡 使用建議

### 新手推薦流程

1. **先觀看 PPO V2**（選項 5）
   - 選擇 8x8 棋盤
   - 觀看 3-5 局遊戲
   - 了解 AI 如何避免碰撞

2. **三方比較**（選項 6）
   - 選擇 8x8 棋盤
   - 了解三種方法的差異
   - 查看改進效果

3. **進階模式**（選項 7）
   - 測試不同棋盤大小
   - 比較不同訓練階段的模型

### 調試/測試流程

1. **訓練新模型後**
   - 使用選項 7 選擇特定模型
   - 測試不同棋盤大小
   - 評估泛化能力

2. **比較訓練版本**
   - 使用選項 7 分別測試
   - checkpoint 模型 vs final 模型
   - best_model vs final 模型

---

## 🐛 常見問題

### Q: "沒有可用的模型"

**A**: 你需要先訓練模型：

```bash
# 訓練 Q-learning
python train.py --mode standard

# 訓練 PPO V1
python snake_ai_ppo.py --mode train --timesteps 100000

# 訓練 PPO V2
python snake_ai_ppo_v2.py --mode train --timesteps 500000
```

### Q: "需要安裝 stable_baselines3"

**A**: PPO 功能需要額外的依賴：

```bash
pip install stable-baselines3 torch
```

如果有 GPU（Python 3.12）：
```bash
pip install torch --index-url https://download.pytorch.org/whl/cu121
```

### Q: 遊戲速度太快/太慢

**A**: 在代碼中調整 `time.sleep()` 值：

```python
# 在 watch_ppo_play() 或 watch_ai_play() 函數中
time.sleep(0.15)  # 調整這個值，越大越慢
```

### Q: 選項 6 比較時間太長

**A**: 可以修改測試回合數：

```python
# 在 compare_all_methods() 函數中
num_games = 10  # 改為 5 或更少
```

### Q: 想在特定棋盤大小訓練的模型上測試

**A**: 使用選項 7 (進階模式)：
1. 選擇 AI 類型
2. 選擇對應的模型文件
3. 選擇相同的棋盤大小

---

## 📈 性能指標說明

### 顯示的統計數據

#### 遊戲結束統計（PPO）
```
最終分數: 15          # 吃到的食物數量
蛇的長度: 16          # 蛇的總長度 (分數 + 1)
總步數: 342           # 移動的步數
總獎勵: 145.30        # 累積的獎勵值
成功率: 15/63 (23.8%) # 分數/最高可能分數
Near-miss: 8          # 接近身體但未撞擊次數 (僅 V2)
```

#### 三方比較統計
- **平均分數**: 多回合的平均表現
- **最高分**: 單回合最佳成績
- **最低分**: 單回合最差成績
- **標準差**: 分數的穩定性（越小越穩定）

---

## 🎓 技術說明

### Q-learning 輸出
顯示每步的 Q 值陣列：
```
步驟   1: 動作=RIGHT  Q值=[  2.45,  -1.20,   3.67,   0.89] 分數=  0
                              ↑      ↑       ↑       ↑
                             UP    LEFT   RIGHT   DOWN
```
Q 值越高，表示該動作越有價值。

### PPO 輸出
不顯示 Q 值（因為使用神經網路策略），但顯示：
- 選擇的動作
- 當前分數和蛇長
- 詳細的遊戲統計

### PPO V2 特別輸出
額外顯示：
- **Near-miss 次數**: 蛇頭接近身體但成功避開的次數
- 這個指標反映 AI 的避障意識

---

## ✅ 快速檢查清單

使用前確認：
- [ ] 已安裝所有依賴 (`pip install -r requirements.txt`)
- [ ] 至少有一個訓練好的模型
- [ ] 如果要使用 PPO，已安裝 `stable_baselines3`

測試新模型時：
- [ ] 先用選項 5 或 7 單獨觀看
- [ ] 檢查分數是否合理
- [ ] 使用選項 6 進行完整比較

報告結果時記錄：
- [ ] AI 類型和版本
- [ ] 棋盤大小
- [ ] 平均分數和標準差
- [ ] 測試回合數

---

## 🔗 相關文檔

- [PPO_V2_README.md](PPO_V2_README.md) - PPO V2 訓練指南
- [V1_VS_V2_COMPARISON.md](V1_VS_V2_COMPARISON.md) - V1 vs V2 詳細對比
- [PPO_README.md](PPO_README.md) - PPO V1 使用指南
- [GPU_SETUP.md](GPU_SETUP.md) - GPU 訓練配置

---

**享受觀看 AI 玩遊戲！** 🐍🎮🤖
