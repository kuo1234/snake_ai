# PPO (Proximal Policy Optimization) 詳細介紹

## 🎯 什麼是 PPO？

PPO（Proximal Policy Optimization，近端策略優化）是由 OpenAI 在 2017 年提出的現代強化學習算法。它已成為深度強化學習領域最流行和最實用的算法之一。

## 🧠 核心概念

### 1. Policy Gradient（策略梯度）

PPO 屬於 **策略梯度（Policy Gradient）** 方法家族：

- **直接學習策略**: 不像 Q-learning 學習價值函數，PPO 直接學習策略網路 π(a|s)
- **神經網路表示**: 使用深度神經網路來參數化策略
- **概率輸出**: 輸出每個動作的概率分布

```
輸入（狀態）→ 神經網路 → 輸出（動作概率）
[12 維特徵] → [隱藏層] → [4 個動作的概率]
```

### 2. Clipped Objective（裁剪目標函數）

PPO 的核心創新是 **裁剪機制**，防止策略更新過大：

```
L^CLIP(θ) = E[min(r(θ)·A, clip(r(θ), 1-ε, 1+ε)·A)]
```

其中：
- `r(θ)` = 新舊策略的概率比
- `A` = 優勢函數（Advantage）
- `ε` = 裁剪範圍（通常 0.1-0.2）

**為什麼需要裁剪？**
- 防止策略突然改變太多
- 保證訓練穩定性
- 避免「災難性遺忘」

### 3. On-Policy Learning（在線學習）

- **收集經驗**: 使用當前策略與環境互動
- **更新策略**: 使用收集的經驗更新神經網路
- **重複過程**: 丟棄舊數據，用新策略收集新經驗

## 🏗️ PPO 架構

### 在 Snake 遊戲中的應用

```
┌─────────────────────────────────────────────────────┐
│                    PPO Agent                        │
├─────────────────────────────────────────────────────┤
│                                                     │
│  輸入層 (12 維)                                      │
│  ├─ 危險檢測 (4): 上下左右是否危險                    │
│  ├─ 食物方向 (4): 食物在上下左右哪個方向               │
│  └─ 當前方向 (4): 當前移動方向 (one-hot)             │
│                                                     │
│  隱藏層 (64 神經元)                                   │
│  └─ ReLU 激活函數                                    │
│                                                     │
│  隱藏層 (64 神經元)                                   │
│  └─ ReLU 激活函數                                    │
│                                                     │
│  輸出層 (4 維)                                       │
│  └─ Softmax → 動作概率 [P(上), P(左), P(右), P(下)]   │
│                                                     │
│  價值估計器 (Value Network)                          │
│  └─ 估計當前狀態的價值 V(s)                           │
│                                                     │
└─────────────────────────────────────────────────────┘
```

### 訓練流程

```
1. 收集經驗
   ├─ 使用當前策略玩遊戲
   ├─ 記錄 (狀態, 動作, 獎勵)
   └─ 累積 2048 步經驗

2. 計算優勢函數
   ├─ 估計狀態價值 V(s)
   ├─ 計算回報 G_t
   └─ 優勢 A = G_t - V(s)

3. 更新策略
   ├─ 計算策略比率 r(θ)
   ├─ 應用裁剪目標
   ├─ 反向傳播
   └─ 重複 10 個 epoch

4. 評估進度
   ├─ 定期測試性能
   └─ 保存最佳模型

5. 重複步驟 1-4
```

## 📊 PPO vs Q-learning

| 特性 | PPO | Q-learning |
|------|-----|-----------|
| **算法類型** | Policy-based | Value-based |
| **學習對象** | 策略 π(a\|s) | Q 值 Q(s,a) |
| **函數逼近** | 深度神經網路 | Q 表（離散）|
| **狀態空間** | 連續/大規模 | 離散/小規模 |
| **動作選擇** | 概率採樣 | ε-貪心 |
| **訓練穩定性** | 高（裁剪機制）| 中等 |
| **並行化** | 容易（多環境）| 困難 |
| **記憶需求** | 高（網路參數）| 低至高（Q 表大小）|
| **適用棋盤** | 6x6 到 20x20+ | 6x6 到 10x10 |
| **訓練時間** | 中等（可並行）| 快（小棋盤）|

## 🎮 在 Snake 遊戲中的表現

### 觀察空間（12 維特徵）

```python
[
    danger_up,      # 0 或 1: 往上是否危險
    danger_down,    # 0 或 1: 往下是否危險
    danger_left,    # 0 或 1: 往左是否危險
    danger_right,   # 0 或 1: 往右是否危險
    
    food_up,        # 0 或 1: 食物在上方
    food_down,      # 0 或 1: 食物在下方
    food_left,      # 0 或 1: 食物在左邊
    food_right,     # 0 或 1: 食物在右邊
    
    dir_up,         # 0 或 1: 當前往上移動
    dir_down,       # 0 或 1: 當前往下移動
    dir_left,       # 0 或 1: 當前往左移動
    dir_right,      # 0 或 1: 當前往右移動
]
```

### 獎勵設計

```python
if 遊戲結束:
    if 贏得遊戲（填滿棋盤）:
        reward = +50.0
    else:
        reward = -10.0  # 撞牆或撞自己
elif 吃到食物:
    reward = +10.0
else:
    reward = +0.1   # 存活獎勵
    
    # 獎勵塑形
    if 朝食物移動:
        reward += 0.5
    elif 遠離食物:
        reward -= 0.5
```

### 訓練參數

```python
PPO(
    policy="MlpPolicy",      # 多層感知器策略
    learning_rate=3e-4,      # 學習率
    n_steps=2048,            # 每次更新收集的步數
    batch_size=64,           # 小批量大小
    n_epochs=10,             # 每批數據訓練的輪數
    gamma=0.99,              # 折扣因子
    gae_lambda=0.95,         # GAE 參數
    clip_range=0.2,          # 裁剪範圍
)
```

## 📈 訓練建議

### 初學者（快速測試）
```bash
python snake_ai_ppo.py --mode train --timesteps 100000 --board-size 6
```
- 棋盤: 6×6
- 訓練步數: 10 萬
- 預計時間: 5-10 分鐘
- 預期效果: 能學會基本的避障和找食物

### 標準訓練
```bash
python snake_ai_ppo.py --mode train --timesteps 500000 --board-size 8
```
- 棋盤: 8×8
- 訓練步數: 50 萬
- 預計時間: 30-60 分鐘
- 預期效果: 平均分數 20-40 分

### 進階訓練
```bash
python snake_ai_ppo.py --mode train --timesteps 1000000 --board-size 10
```
- 棋盤: 10×10
- 訓練步數: 100 萬
- 預計時間: 1-2 小時
- 預期效果: 平均分數 40-60 分

## 🔧 調參建議

### 提高學習速度
```python
learning_rate=5e-4       # 增加學習率（默認 3e-4）
n_envs=8                 # 增加並行環境（默認 4）
```

### 提高穩定性
```python
learning_rate=1e-4       # 降低學習率
clip_range=0.1           # 減小裁剪範圍（默認 0.2）
```

### 提高探索
```python
ent_coef=0.01           # 增加熵係數（鼓勵探索）
```

## 📊 監控訓練進度

使用 TensorBoard 查看訓練指標：

```bash
tensorboard --logdir logs/ppo_snake
```

重要指標：
- **ep_len_mean**: 平均回合長度（越長越好）
- **ep_rew_mean**: 平均回合獎勵（越高越好）
- **value_loss**: 價值函數損失（應該下降）
- **policy_loss**: 策略損失（應該下降）
- **entropy**: 策略熵（保持一定水平，避免過早收斂）
- **approx_kl**: KL 散度（監控策略變化幅度）

## 🎯 優勢與局限

### ✅ 優勢

1. **穩定訓練**: 裁剪機制保證穩定性
2. **高效採樣**: 可重複使用數據多次更新
3. **並行化**: 支援多進程訓練
4. **擴展性**: 適合大規模問題
5. **實用性**: 工業界廣泛應用
6. **易於使用**: stable_baselines3 提供現成實現

### ⚠️ 局限

1. **計算資源**: 需要 GPU 加速（可選但推薦）
2. **訓練時間**: 需要較長時間達到最優
3. **超參數**: 需要調整超參數獲得最佳效果
4. **採樣效率**: 相比 DQN 等 off-policy 方法採樣效率較低
5. **記憶體**: 神經網路需要較多記憶體

## 🚀 進階優化方向

1. **網路架構**
   - 增加隱藏層數量和大小
   - 嘗試 LSTM/GRU 處理部分可觀察性
   - 使用 CNN 處理視覺輸入

2. **獎勵設計**
   - 更細緻的獎勵塑形
   - 基於距離的連續獎勵
   - 懲罰循環移動

3. **課程學習**
   - 從小棋盤逐步增加到大棋盤
   - 從短目標逐步增加難度

4. **其他算法**
   - A2C/A3C: 異步版本
   - SAC: 連續動作空間
   - DQN: 經驗回放

## 📚 參考資源

- [PPO 原始論文](https://arxiv.org/abs/1707.06347)
- [Stable-Baselines3 文檔](https://stable-baselines3.readthedocs.io/)
- [OpenAI Spinning Up](https://spinningup.openai.com/)
- [深度強化學習課程](https://www.youtube.com/watch?v=2pWv7GOvuf0)

---

現在你已經了解 PPO 的原理，可以開始訓練你的 Snake AI 了！

```bash
# 開始訓練
python snake_ai_ppo.py --mode train --timesteps 100000

# 觀看訓練結果
python demo_ai.py
# 選擇選項 4 或 5
```

祝訓練成功！🐍🎮🤖
