# V3 增强修改 - 解决 Stage 2 卡死问题

## 🎯 问题诊断

### 现象
- ✅ **Stage 1 (6x6)**: 完美！可以玩到满分
- ❌ **Stage 2 (8x8)**: AI 又开始把自己卡死

### 根本原因
**策略转移失败 - AI 变得"短视贪婪"**

1. **Stage 1 的成功策略**:
   - 靠近食物: +0.3
   - 陷阱惩罚: -1.5
   - 风险/收益比: 5x（谨慎但不过分）
   - AI 学会: "安全第一，贴边移动"

2. **Stage 2 的新规则**:
   - 靠近食物: +1.0 ⬆️⬆️⬆️（诱惑太大！）
   - 陷阱惩罚: -3.0（惩罚太轻！）
   - 风险/收益比: 3x（AI 觉得值得冒险）
   - AI 计算: "为了 +1.0 的高额奖励，冒 30% 卡死风险也值得！"

**结论**: AI 并非不会避免卡死，而是**选择不避免**，因为吃食物的回报太诱人了。

---

## ✅ 实施的解决方案

### 方案一：大幅提高陷阱惩罚 (The Stick) ⭐⭐⭐

#### 修改位置
**文件**: `envs/gym_snake_env_v3.py`  
**行数**: 第 376-403 行

#### 具体变更

| Stage | 陷阱惩罚 | 修改前 → 修改后 | 风险/收益比 |
|-------|---------|----------------|------------|
| Stage 1 (6x6) | `-1.5` | 不变 | 5x |
| Stage 2 (8x8) | `-3.0` → **`-10.0`** | ⬆️ 3.3倍 | 10x ⬆️ |
| Stage 3 (10x10) | `-5.0` → **`-15.0`** | ⬆️ 3倍 | 15x ⬆️⬆️ |
| Stage 4 (12x12) | `-5.0` → **`-15.0`** | ⬆️ 3倍 | 30x ⬆️⬆️⬆️ |

#### 代码变更
```python
# envs/gym_snake_env_v3.py 第 376-403 行
reward_config = {
    1: {
        'trap_penalty': -1.5,   # 不变
        # ...
    },
    2: {
        'trap_penalty': -10.0,  # 从 -3.0 提高到 -10.0 ⬆️
        # ...
    },
    3: {
        'trap_penalty': -15.0,  # 从 -5.0 提高到 -15.0 ⬆️⬆️
        # ...
    },
    4: {
        'trap_penalty': -15.0,  # 从 -5.0 提高到 -15.0 ⬆️⬆️
        # ...
    }
}
```

#### 预期效果
- ✅ 卡死的代价（-10.0）远大于靠近食物几步的收益（+1.0 × 3步 = +3.0）
- ✅ AI 必须学会评估"未来的空间"而非"眼前的食物"
- ✅ 强制 AI 重新权衡短期收益 vs 长期生存

---

### 方案三：强化参数震荡 (Hyperparameter Shock) ⭐⭐⭐

#### 修改位置
**文件**: `snake_ai_ppo_v3.py`  
**行数**: 第 365-408 行

#### 问题
原有的参数震荡只提高学习率（LR），但**探索率（entropy）未提高**，导致 AI "懒得"探索新策略。

#### 增强方案
三阶段渐进式震荡：

| 阶段 | 步数 | 学习率 | 探索率 | 目的 |
|------|------|--------|--------|------|
| **阶段 1: 震荡** | 150k | 3e-4 | 0.02 | 强制重新探索，打破旧策略 |
| **阶段 2: 过渡** | 100k | 1.5e-4 | 0.015 | 稳定新策略，减少震荡 |
| **阶段 3: 稳定** | 250k | 原值 | 原值 | 精细优化，收敛到最优 |

#### 代码变更
```python
# snake_ai_ppo_v3.py 第 365-408 行

# 阶段 1: 高学习率 + 高探索率震荡（150k 步）
model.learning_rate = 3e-4   # 提高学习率
model.ent_coef = 0.02        # 提高探索率（原始约 0.01）⬆️ 新增
model.learn(total_timesteps=150_000, ...)

# 阶段 2: 中等学习率 + 中等探索率过渡（100k 步）⬆️ 新增
model.learning_rate = 1.5e-4
model.ent_coef = 0.015
model.learn(total_timesteps=100_000, ...)

# 阶段 3: 恢复正常参数
model.learning_rate = original_lr
model.ent_coef = original_ent
```

#### 预期效果
- ✅ 高探索率强制 AI 尝试新路径
- ✅ AI 会"发现"卡死的惩罚变重了（-10.0）
- ✅ 三阶段渐进避免震荡过度导致崩溃
- ✅ 总震荡步数从 100k 增加到 250k（更充分）

---

## 📊 理论分析

### 风险/收益比对比

| Stage | 旧惩罚 | 新惩罚 | 食物奖励 | 旧比率 | 新比率 | 提升 |
|-------|--------|--------|---------|--------|--------|------|
| 1 | -1.5 | -1.5 | +0.3 | 5x | 5x | - |
| 2 | -3.0 | **-10.0** | +1.0 | 3x | **10x** | ⬆️ 3.3倍 |
| 3 | -5.0 | **-15.0** | +1.0 | 5x | **15x** | ⬆️ 3倍 |
| 4 | -5.0 | **-15.0** | +0.5 | 10x | **30x** | ⬆️ 3倍 |

### AI 决策逻辑变化

#### 修改前（Stage 2）
```
AI 思考:
"靠近食物 3 步 = +3.0
 陷阱惩罚 = -3.0
 → 只要有 51% 成功率就值得冒险！"
```
**结果**: AI 频繁陷入死路 ❌

#### 修改后（Stage 2）
```
AI 思考:
"靠近食物 3 步 = +3.0
 陷阱惩罚 = -10.0
 → 必须有 77% 成功率才值得冒险！
 → 否则期望值为负：0.5×3.0 + 0.5×(-10.0) = -3.5"
```
**结果**: AI 会更谨慎评估空间 ✅

---

## 🧪 测试验证

### 测试脚本
`test_enhanced_penalties.py` 已创建

### 测试结果
```
Stage 1: 风险/收益比 = 5.0x  (适中，学习基础)
Stage 2: 风险/收益比 = 10.0x (谨慎，主动追食但不贪婪)
Stage 3: 风险/收益比 = 15.0x (极度谨慎，空间管理优先)
Stage 4: 风险/收益比 = 30.0x (极度保守，长期生存)
```

---

## 🚀 下一步行动

### 1. 重新训练 Stage 2
```bash
# 方法 A: 从头开始训练 Stage 2（推荐）
rm -rf models/ppo_snake_v3_curriculum/Stage2_Intermediate
python snake_ai_ppo_v3.py --mode train --start-stage 1

# 方法 B: 继续从 Stage 1 迁移训练
python snake_ai_ppo_v3.py --mode train --start-stage 1
```

### 2. 观察训练指标

**关键指标监控**:
- ✅ **平均分数**: 应该稳步上升
- ✅ **死亡原因**: "陷阱死亡"应该大幅减少
- ✅ **边缘比例**: Stage 2 应该 < 30%（不再过度贴边）
- ✅ **步数**: 应该增加（AI 更谨慎地探索）

### 3. 评估效果

训练完成后：
```bash
# 评估 Stage 2
python snake_ai_ppo_v3.py --mode eval --stage 1

# 可视化观察
python demo_ui_v3.py  # 选择 Stage 2
```

**成功标准**:
- ✅ Stage 2 平均分数 ≥ 40
- ✅ 卡死次数 < 10%
- ✅ AI 会绕路而非直冲食物

---

## ⚠️ 可能的副作用

### 情况 1: AI 变得过于胆小
**症状**: Stage 2 分数反而下降，AI 不敢动

**诊断**: 陷阱惩罚可能过重（-10.0 太高）

**解决方案**: 
```python
# 降低 Stage 2 陷阱惩罚
'trap_penalty': -7.0,  # 从 -10.0 降到 -7.0
```

### 情况 2: 参数震荡导致性能下降
**症状**: Stage 2 训练后性能不如 Stage 1

**诊断**: 震荡步数太多（250k），破坏了原有策略

**解决方案**:
```python
# 减少震荡步数
# 阶段 1: 100k（从 150k）
# 阶段 2: 50k（从 100k）
# 总计: 150k（从 250k）
```

### 情况 3: AI 学会了但速度很慢
**症状**: Stage 2 能达标但需要很多步

**诊断**: 陷阱惩罚正确，但缺乏积极性

**解决方案**（方案二）:
```python
# 保持陷阱惩罚，但降低食物奖励
'approach': 0.7,  # 从 1.0 降到 0.7
'away': -0.4,     # 从 -0.5 升到 -0.4
```

---

## 📝 总结

### 核心思想
**"恐惧 > 贪婪"** - 让 AI 重新"害怕"卡死

### 实施方案
1. ✅ **提高陷阱惩罚**: -3.0/-5.0 → -10.0/-15.0
2. ✅ **强化参数震荡**: 增加探索率，三阶段渐进
3. ⏳ **观察效果**: 重新训练 Stage 2 并评估

### 预期结果
- ✅ Stage 2 不再因贪吃而卡死
- ✅ AI 学会评估长期空间而非短期食物
- ✅ 风险/收益比从 3x 提升到 10x

### 回滚计划
如果效果不佳，可以：
1. 降低陷阱惩罚到 -7.0（折中方案）
2. 减少震荡步数到 150k
3. 或实施方案二（降低食物奖励）

---

更新时间: 2025-10-26  
版本: V3 Enhanced - Anti-Greed Patch  
状态: ✅ 已实施，等待训练验证
